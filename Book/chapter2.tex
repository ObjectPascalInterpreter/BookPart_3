{\bfseries\slshape\sffamily\color{ChapterTitleColor} \chapter{Internal Changes}} \label{chap:internalChanges}

\section{Introduction}

Before we embark on building the new parser, we should first talk about the internal structure of the new modules.

In order to deal with modules, allowing user functions to be first class and having a very basic object model for lists and strings, quite a few internal changes were made to the code compared to version 2. In the new version the basic unit is the module. Every module has a name which can be assigned by the user. The only modules that have fixed names is the main module, called {\tt _main_}, and a series of built-in modules such as {\tt math, os}, etc.

\begin{figure}[htpb]
\centering
\includegraphics[scale=0.45]{structureModule_1.pdf}
\caption{Internal structure of a module.}
\label{fig:StructureModule1}
\end{figure}

The internal structure of a module is given in Figure~\ref{fig:StructureModule1}. The three important components are the code block, the constant table and the symbol table. User functions have a similar structure but without a constants table (They use the module level constants table). The code block is where bytecode is stored, the constant table is where literal constants such as doubles, lists and strings are stored, and finally the symbol table stores any variables that the user might have introduced, including user functions.

%At startup, a main module, called {\tt _main_}, is created. At the interactive console, users are sitting within this module.
%
%There are two scenarios one can envisage from the user point of view (Figure~\ref{fig:terminalConsole}). Either we are at the Rhodus interactive console or the terminal command line. At the Rhodus terminal we would type in some code and the code is immediately executed:
%
%\begin{lstlisting}
%>>a = "A message"
%\end{lstlisting}
%
%or from the terminal command line, we pass to Rhodus the name of a file that contains the code to run:
%
%\begin{lstlisting}
%rhodus runme.rh
%\end{lstlisting}
%
%Depending on the scripts we run, some output may appear at the terminal.

%From the terminal, Rhodus first creates the main module, compiles the code into the main module and runs whatever that code is. Running code from the console is similar with some slight differences. When we launch the Rhodus console, it will create a main module before issuing a prompt to wait for user input. The user now enters a line of Rhodus code, for example: {\tt a = "A message"}. Rhodus takes the text, compiles it and copies the generated byte code into the main module's code block then runs the code block. The difference here is that a user can now write another line of code, for example {\tt println (a)} and hit enter. What happens next is that the new code is compiled, the old code block is cleared and the new byte code copied to the code block. This can be repeated as many times as we like. When calling Rhodus from the terminal command line, the code is loaded run and the program exits.
%
%\begin{figure}[htpb]
%\centering
%\includegraphics[scale=0.45]{terminalConsole.pdf}
%\caption{Difference between running Rhodus from the terminal or from the Rhodus console.}
%\label{fig:terminalConsole}
%\end{figure}

%When a Rhodus program runs, it first creates the main module. Running a module simple means running whatever code it finds in the module's code block. Figure~\ref{fig:StructureModule1} shows a code block inside a module.


When you start up the Rhodus repl (i.e the interactive console), Rhodus first creates the main module, {\tt _main_} then waits for the user to type in code. When the user enters code, its gets compiled into the main module code block. Control is then handed over to the virtual machine to actually run the code. Once that is complete, control returns to wait for the user to type in more code and so the cycle repeats.

The constant factor in all this are the symbol and constant values tables, more precisely the module level symbol and constants table which we can see in Figure~\ref{fig:StructureModule1}. Whenever we type something like {\tt a = 1.2}, its first gets converted into code, the code is run which causes the symbol {\tt a} to appear in the symbol table along with the value {\tt 1.2} in the constants value table. In essence, what code does is manipulate the symbol table. This makes the symbol table a very important structure in an interpreter.

The final thing left to mention in Figure~\ref{fig:StructureModule1} are the user functions. The figure only shows one but there could be many of these in a single module. User functions are almost little modules within the big module. They have their own symbol table and code block. The big difference is how the user function stores its local symbols and the fact that a user function can accept inputs and of course return values. The module level and user function symbol tables operate in quite different ways. The module level symbol table retrieves symbols by name. Thus with the code {\tt a = 1.2}, a new symbol is created with the name {\tt a} and the number {\tt 1.2}. If we ever want to reference that symbol we locate it using its name {\tt a}. In Rhodus we store the module level symbol tables using a dictionary which makes it reasonably fast to locate a given symbol.

The user function symbol tables are different. Symbols in a user function are stored using an integer index that references locations on the runtime stack. One would imagine that this would make accessing symbols in a user function faster compared but in practice it appears not to be the case, suggesting that the dictionary lookup at the module level is quite fast.

The class below shows the module structure in code:

\begin{lstlisting}
   // Base module class. 
   TModule = class (TObject)
       name : string;
       moduleProgram : TProgram;  // Contains code block and constants table
       symbolTable : TSymbolTable;
       helpStr : string;

       function    getSize : integer;
       function    find (name : string) : TSymbol;
       procedure   clearCode;
       constructor Create (name : string);
       destructor  Destroy; override;
   end;
\end{lstlisting}   
   
\section{Updates to the Byte Codes}

There are some updates to the byte codes to accommodate the new changes. The load and store opCodes are changed and expanded to those shown in Table~\ref{tbl:loadstore}. One pair, the load and save symbols, are used when the symbols are in the currently running module. The second pair, the load and save attr opCodes are used when a module needs to access symbols in a different module. For example, the following code:

\begin{lstlisting}
a = b
\end{lstlisting}

will first use {\tt load} to push the value of {\tt b} obtained from the current module symbol table onto the stack and then use {\tt save} to store the value that's on the stack to the symbol {\tt a}, also in the symbol table of the current module. In contrast, for the code:

\begin{lstlisting}
a = m1.b
\end{lstlisting}

will first use {\tt load} to push the value of {\tt m1} onto the stack. {\tt loadAttr} will then be used to pop off the value of {\tt m1} and load the value of {\tt b} in module {\tt m1} onto the stack. {\tt saveSymbol} will store the value on the stack to the symbol {\tt a} that resides in the symbol table of the current module. The main difference is that the secondary opCodes expect the stack to hold a reference to a module, where as the symbol opCodes will use the current module.

The reason for having two types of load and save was to try to improve the performance of code accessing symbols within the module the code resides. To be completely symmetric we could have insisted that any code accessing symbols in the module it resides in, should also push a reference of the module onto the stack and then used the attr opCodes to access the symbol. However this would have slowed down the load and store operation within a module. Instead, before we run a code block we pass a reference to the module the code belongs to so that {\tt load} and {\tt save} know what module the current module is.

\begin{table}
\centering
\begin{tabular}{lp{8.5cm}} \toprule
OpCode & Description \\ \midrule
{\tt load} & Load  symbol in the current module onto the stack. \\
{\tt save} & Store the current item on the stack to a symbol in the current module. \\
{\tt loadAttr} & Pop a module reference from the stack and load value in that module onto the stack. \\
{\tt saveAttr} & Pop a module reference from the stack and store the value to the symbol in that module.  \\ \bottomrule
\end{tabular}
\caption{New load and store opCodes}
\label{tbl:loadstore}
\end{table}

The only other new opCode is {\tt importModule} and this is called when the compiler encounters the Rhodus code {\tt import moduleName}. The other minor change is that the {\tt bultin} OpCode has been merged with the {\tt call} opCode. Overall there haven't been many changes at all to the set of OpCodes we use in the virtual machine.


\section{Examples of Byte Code Programs}

I think the best way to describe the operation of the interpreter is to look at some bytecode programs. The first thing to note is that the Rhodus virtual machine is a stack based machine. That is all operations occur around a stack. For example, to add two numbers, $2 + 3$, we first push the operands 2 and 3 onto the stack, then apply the addition operator and push the result back onto the stack. What's left on the stack is the sum. The stack is used as a temporary place to hold data to which operations are applied. As simple as it is, the stack turns out to be an incredible versatile structure. So versatile that we can build a general purchase computer around one. Interestingly, real hardware based computers such as the ARM or Intel microprocessors are not stack based but register based. The reason for the dominance of register based machines is a combination of historical accident and better performance.

Let's start with some simple examples such as assigning a value to a variable. The following Table~\ref{code:simpleExpressions}, shows four such examples. One key instruction is {\tt store a}. This takes whatever is on the stack and saves it to a variable called {\tt a}. The other key instruction is {\tt load b}. This pushes onto the stack whatever value is stored in variable {\tt b}. The variables {\tt a} and {\tt b} along with the values associated with them are kept in the symbol table. The other opcode we've seen before is {\tt pushi} which pushes a literal integer onto the stack. 

We already see at this simple stage, an active interaction between the virtual machine and the symbol table. Most operations in the virtual machine either involve the stack or the symbol table with data moving back and forth between the two as a program executes. Some instructions will manipulate the stack while others will move data between the stack and the symbol table. For example, arithmetic operations only involve operations on the stack. A few instructions just manipulate the symbol table, for example {\tt inc} and {\tt dec}.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{lp{2cm}|lp{2cm}} \toprule
Code & Bytecode & Code & Bytecode \\ \midrule
{\tt a = 2} & {\tt
  0  pushi 2\linebreak
  1  store a
} & {\tt a = 2 + 3} & {\tt
  0  pushi 2\linebreak
  1  pushi 3\linebreak
  2  add\linebreak
  3  store a\linebreak}  \\ \midrule
{\tt a = 2 + 3} & {\tt
  0  pushi 2\linebreak
  1  pushi 3\linebreak
  2  add\linebreak
  3  store a\linebreak
} & {\tt b = a * 2} & {\tt
  0  pushi 2\linebreak
  1  load a\linebreak
  2  mult\linebreak
  3  store a\linebreak
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated simple expressions}.
\label{code:simpleExpressions}
\end{table}

With just three bytecodes, {\tt push}, {\tt load}, {\tt store} and the arithmetic operations such as {\tt add} and {\tt mult} we can do a lot. However, computer programs need to do one other thing to make them really useful, that is conditional branching. For that, we have a number additional bytecodes such as {\tt isGt}, and {\tt jmpIfTrue}. Add to that we also need some Boolean operations, in particular {\tt OR}, {\tt AND} and {\tt NOT}. Given that we need to deal with numbers as well as boolean values, the stack needs to be able to handle different data types. That is, not only numbers, but also data such as True and False values. We could of course model True and False using the numbers 1 and 0. By convention, 0 is often considered False and 1, True. Using such numbers is in fact what the computer does at the hardware level. But being human we like to deal with higher level concepts and since Object Pascal has the notion of True and False we might as well use these rather than just 0 and 1.

With this set of bytecodes we could probably do almost everything. All higher level constructs in a computer language can be described using only these bytecodes. The only thing that perhaps can't be done is implementing subroutines, but perhaps with enough imagination one could even do that. However, rather than being so sparse with our list of bytecodes, most stack based virtual machines supplement the list with a range of other instructions such as calling subroutines and in particular indexing operations. For now lets see we what we can do with the minimal set of bytecodes. In part II of the series there was some discussion of how loops and conditional statements were dealt with we didn't discuss the bytecode that could represent such structures. this is what we'll do here.

\subsubsection*{Repeat/Until}

Let's first consider the repeat/until loop as its probably the simplest. We'll really make it simple by just considering the statement: {\tt repeat until True}, that is no content to the loop itself. The bytecode for this is surprisingly simple, just two instructions, Table~\ref{code:repeatempty}.

The first instruction {\tt push True} pushes True onto the stack. The second instruction is a relative jump instruction. It pops the value off the stack and checks to see if the value is False. If False it jumps -1 instructions. The minus means it jumps back which takes it back to the {\tt push True} instruction, and we start again. Luckily the until statement expression is {\tt True}, so that it will not jump back and get us out of the loop. If we'd set the expression to {\tt False} we would have gone into an infinite loop.


\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{4.6cm}p{4.6cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt repeat \linebreak
until True \linebreak} & {\tt
  0  push True \linebreak
  1  jmpIfFalse -1 \linebreak
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for an empty {\tt repeat} loop}.
\label{code:repeatempty}
\end{table}

What happens if we put a simple statement inside the repeat loop, such as {\tt a = 7}? The result is shown in Table~\ref{code:repeatEmpty}. The number of instructions expands to four. The extra two instructions are in front of the {\tt push True}. The {\tt jmpIfFalse} has been modified to jump back three steps corresponding to the start of the {\tt a = 7} statement.


\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{4.6cm}p{4.6cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt repeat \linebreak
\phantom{A} a = 7 \linebreak
until True \linebreak} & {\tt
  0  push 7 \linebreak
  1  store a \linebreak
  2  push True \linebreak
  3  jmpIfFalse -3 \linebreak
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a non-empty {\tt repeat} loop}.
\label{code:repeatEmpty}
\end{table}

\subsubsection*{While Loop}

Next in line is the while loop. Let's first look at an empty while loop, such as {\tt while False do end}.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{4.6cm}p{4.6cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt while False do \linebreak
end \linebreak} & {\tt
  0  push False \linebreak
  1  jmpIfFalse 2 \linebreak
  2  jmp -2
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for an empty {\tt while} loop}.
\label{code:for}
\end{table}

It doesn't take many instructions to implement a while loop, just three. We start by pushing False from {\tt while False do} onto the stack. The next instruction, {\tt jmpIfFalse}, is used to check for the result of the while expression. If false it jumps forward two instructions. If it does this its jumps right out of the while loop. In the process it jumps over the last instruction which is an unconditional {\tt jmp} which jumps us two instructions back to the beginning and so we start again. The initial instruction that pushes {\tt False} onto the stack would in practice be the result of evaluating a while expression such as {\tt a > 4} which would evaluate to {\tt True} or {\tt False}. If we add actual statements inside the while loop, these will generate bytecode between line 0 and line 1 with a corresponding change to the jump distances. As an example, Table~\ref{code:whilePractical}, shows a while loop with an assignment in the body of the loop. The first three instructions concern themselves with evaluating {\tt a > 5}. After than its the same as before except we have a different body statement: {\tt a = a - 1}.


\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{4.6cm}p{4.6cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt while a > 5 do \linebreak
\phantom{A} a = a - 1 \linebreak
end \linebreak} & {\tt
  0  load a \linebreak
  1  pushi 5 \linebreak
  2  isGt \linebreak
  3  jmpIfFalse 6 \linebreak
  4  load a \linebreak
  5  pushi 1 \linebreak
  6  sub \linebreak
  7  store a \linebreak
  8  jmp -8
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a non-empty {\tt while} loop}.
\label{code:whilePractical}
\end{table}

\subsubsection*{For Loop}

A much more complicated loop to model in bytecode is the {\tt for} loop. To keep things simple lets again consider an empty loop first such as {\tt for i = 1 to 5 do end}. This is shown in Table~\ref{code:for}.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{4.6cm}p{4.6cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt for i = 1 to 5 do \linebreak
end \linebreak} & {\tt
  0  pushi 1 \linebreak
  1  store i \linebreak
  2  load i \linebreak
  3  pushi 5 \linebreak
  4  isGt \linebreak
  5  jmpIfTrue 3 \linebreak
  6  inc i, 1 \linebreak
  7  jmp -5
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a {\tt for} loop}.
\label{code:for}
\end{table}

The {\tt for} loop requires nine instructions to implement which will make the {\tt for} loop slow to execute. This is a situation where adding specialist bytecode might improve performance. For example, we could introduce a special {\tt doFor} bytecode that would handle some of the incrementing and testing. 

The first thing the code does, Table~\ref{code:for}, is initialize the loop variable {\tt i}. It never executes this again. The loop proper stats at line 2 where it loads the value of the loop variable and the upper limit value, 5. It compares the two using {\tt isGT}, standing for isGreaterThan, and pushes the boolean result onto the stack. In line 5 we use a {\tt jmpIfTrue} to pop off the boolean result and if True we jump forward three instructions which essentially jumps us completely out of the loop. However, if the result is False we move to the next instruction which increments the {\tt i} variable by one. Note that the {\tt inc} instruction modifies the variable in the symbol table and neither pops or pushes anything to and from the stack. Finally, on line 7 we jump back five instructions to line 2 and start the process again. One obvious place we could optimize the code is in line 4 and 5. We could easily replace these two instructions with a single {\tt jmpIfGt}. We'll consider such optimizations in a later chapter.

\subsection*{If Statement}

The {\tt if} statement is reasonably straightforward. Without the {\tt else} clause, Table~\ref{code:if}, the code initially pushes the result of the {\tt if} evaluation onto the stack. If the value is false we jump three instructions which gets us out the {\tt if} statement. If not, we continue, which results in the body of the {\tt} statement being executed.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{5.5cm}p{5.5cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt if True then \linebreak
\phantom{A} a = 14 \linebreak
end \linebreak} & {\tt
  0  push True \linebreak
  1  jmpIfFalse 3 \linebreak
  2  pushi 14 \linebreak
  3  store a
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a simple {\tt if} statement without the {\tt else} clause}.
\label{code:if}
\end{table}

When we add the {\tt else} clause, Table~\ref{code:ifElse}, the initial jump after the {\tt if} test, is to the start of the {\tt else} code. If the {\tt if} statement if {\tt True}, then the code continues in the body of the {\tt if} statement. In line 4, you'll see an unconditional {\tt jmp} instructions. This is is jump over the code for the {\tt else} clause and out of the {\tt if} statement completely.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{5.3cm}p{5.3cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt if True then \linebreak
\phantom{A} a = 14 \linebreak
else  \linebreak
\phantom{A} a = 26  \linebreak
end \linebreak} & {\tt
  0  push True \linebreak
  1  jmpIfFalse 4 \linebreak
  2  pushi 14 \linebreak
  3  store a \linebreak
  4  jmp 3 \linebreak
  5  pushi 26 \linebreak
  6  store a
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a simple {\tt if} statement with an {\tt else} clause}.
\label{code:ifElse}
\end{table}


\subsubsection*{Switch Statement}

By far the most complex construct to consider is the switch statement which involves thirteen core instructions for a switch with two case options, Table~\ref{code:Switch}. Two new instructions have been introduced to deal with the switch statement, {\tt dup} and {\tt popdup}. {\tt dup} duplicates the current top entry on the stack. For example, if the top of the stack has an integer value 3, after dup we will have two stack entries with value 3. The second new bytecode, {\tt popdup} just pops the stack entry. This is to remove the duplicated stack entry that was introduced by {\tt dup}. We could have just used {\tt pop} but I used {\tt popdup} to help remind me what was going on in the code. The switch statement itself is implemented as a series of tests to compare the switch value with each case value. This is not a very efficient approach. For example, imagine you have 100 case statements, and the switch value is 100, the current method would have to do 99 comparisons before it reached 100. From a practical point of view this isn't the way to implement a switch statement but its advantage is that it's not difficult to implement. A better approach is to use a jump table. We'll consider this in a later chapter.

Considering the example in Table~\ref{code:Switch}, you'll see two case options and the corresponding two {\tt jmpIftrue} instructions

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{5.4cm}p{5.4cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt switch 1 \linebreak
\phantom{A} case 1 : a = 14 \linebreak
\phantom{A} case 2 : a = 23 \linebreak
end \linebreak} & {\tt
  0  pushi 1 \linebreak
  1  dup \linebreak
  2  pushi 1 \linebreak
  3  isEq \linebreak
  4  jmpIfTrue 6 \linebreak
  5  dup \linebreak
  6  pushi 2 \linebreak
  7  isEq \linebreak
  8  jmpIfTrue 5 \linebreak
  9  jmp 7 \linebreak
 10  pushi 14 \linebreak
 11  store a \linebreak
 12  jmp 4 \linebreak
 13  pushi 23 \linebreak
 14  store a \linebreak
 15  jmp 1 \linebreak
 16  popDup
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a {\tt switch} statement without an {\tt else} clause}.
\label{code:Switch}
\end{table}

If we add an else clause to the switch construct, Table~\ref{code:SwitchfElse}, we see a final section to the code that supports {\tt else}, where we set {\tt a = 45}.

\begin{table}
\centering
\begingroup\setlength{\fboxsep}{0pt}
\colorbox{mylightgray}{%
\begin{tabular}{p{5.1cm}p{5.1cm}} \toprule
Code & Bytecode  \\ \midrule
{\tt switch 1 \linebreak
\phantom{A} case 1 : a = 14 \linebreak
else \linebreak
\phantom{A}  a = 45 \linebreak
end \linebreak} & {\tt
  0  pushi 1 \linebreak
  1  dup \linebreak
  2  pushi 1 \linebreak
  3  isEq \linebreak
  4  jmpIfTrue 2 \linebreak
  5  jmp 4 \linebreak
  6  pushi 14 \linebreak
  7  store a \linebreak
  8  jmp 3 \linebreak
  9  pushi 45 \linebreak
 10  store a \linebreak
 11  popDup
}  \\ \bottomrule
\end{tabular}}\endgroup
\caption{Code generated for a {\tt switch} statement with an {\tt else} clause}.
\label{code:SwitchfElse}
\end{table}

In summary, the minimal number of bytecodes we introduced previously together with the addition of one or two others is sufficient to implement any looping syntax you'd care to invent. For example, it wouldn't be hard to put together bytecode to mimic loops like {\tt a = 10; repeat a = a - 1 while a > 5}. There are also many variants on the for loop we could also implement.

\subsubsection*{Indexing Support}

Arrays and lists are always an important part of any programming language where a common operation is indexing. For example if we had a list such as:

\begin{lstlisting}
>> a = [1,2,3,4]
\end{lstlisting}

We would want to either access or set a particular element using this syntax:

\begin{lstlisting}
>> a = [1,2,3,4]
>> b = a[1]
>> a[1] = 99
\end{lstlisting}

The same applies to arrays. We only have to introduce two new bytecodes to support this functionality, they are:

{\tt lvecIdx} To access an element from an array or list

{\tt svecIdx} To store a value to an index array or list.

These were introduced in part II of the series and were described there in some detail. As a reminder {\tt lvecIdx} expect two items to be on the stack, the index followed by the object itself. In part II the object could be either a list or a string. In part III we extend this to include arrays. The store bytecode, {\tt svecIdx}  expects three items to be on the stack, the index, the object itself (array, list or string), and finally the value to store at the indexth position.

One thing we've not mentioned so far is what happens to user functions? Because user functions access variables differently, we need a parallel set of bytecodes for user functions. In this case we have {\tt localLvecIdx} and {\tt localSvecIdx}, the former for accessing and the later for storing. The same applies to the load and store bytecode we talked about in the last section. For user functions we'll need {\tt loadLocal} and {\tt storeLocal}. Semantically these parallel bytecodes do the same thing but under the hood, one set accesses variables by name and the other, the local ones, access variables by index. This is because all variables (other than global) that are part of a user function are stored on the stack and can therefore be indexed directly via the stack.

One last bytecode we need for lists, is {\tt createList}, this was also discussed in part II and is used to construct a list at runtime. All the elements of the list are expected to be on the stack. The number of items in the list is stored in the {\tt createList} operand field. Let's see some examples of bytecode that uses lists.

Creating a list:

\begin{lstlisting}
a = [1,2,3]
\end{lstlisting}

This statement would generate the following bytecode:

\begin{lstlisting}
  0  pushi 1
  1  pushi 2
  2  pushi 3
  3  createList 3
  4  store a
\end{lstlisting}

Things to note, the {\tt createList} bytecode also includes the number of items, in this case 3. The items are expected to be on the stack so that {\tt createList} can just pop them off. One small issue, the items when popped come off in reverse order compared to the original Rhodus statement, where the order was {\tt [1,2,3]}. You'll notice in the bytecode that 3 is popped of first so that the order {\tt createList} gets is {\tt 3,2,1}. This isn't a real problem and we just have to make sure that the popped items get put into the right locations in the new list. This is most easily done with a {\tt for/downto} loop.

For a nested list, such as:

\begin{lstlisting}
a = [[1,2],[3,4]]
\end{lstlisting}

we'd have the following bytecode program:

\begin{lstlisting}
  0  pushi 1
  1  pushi 2
  2  createList 2
  3  pushi 3
  4  pushi 4
  5  createList 2
  6  createList 2
  7  store a
\end{lstlisting}

The way this is done, means we can have lists nested to any depth. First lines 1, 2 and 3 create the sublist {\tt [1,2]}, note that when {\tt createList} execute it leaves the new list on the stack. Next we see the sublist {\tt [3,4]} being made, again, the {\tt createList} leaves the sublist on the stack. At this point we have two sublists on the stack. When we get to the final {\tt createList} instruction, it pops the two sublists from the stack and puts them into a new list, forming {\tt [[1,2],[3,4]]}, again it leaves this list on the stack but the final instruction, {\tt store a}, pops the list and stores it in {\tt a}.

If you've ever disassembled Python bytecode in relation to lists, you'll see it uses the same approach to handle lists. Out of interest here is the python bytecode for the statement: {\tt a = [[1,2],[3,4]]}:

\begin{lstlisting}
// Python Bytecode
  0 LOAD_CONST     0 (1)
  1 LOAD_CONST     1 (2)
  2 BUILD_LIST     2
  3 LOAD_CONST     2 (3)
  4 LOAD_CONST     3 (4)
  5 BUILD_LIST     2
  6 BUILD_LIST     2
  7 STORE_NAME     0 (a)
\end{lstlisting}

The only difference is in the names of the bytecodes, {\tt LOAD\_CONST} for {\tt pushi}, {\tt BUILD\_LIST} for {\tt createList} and {\tt STORE\_NAME} for {\tt store}. I should mention that the Rhodus bytecode wasn't modelled on Python's bytecode but similar solutions popup repeatedly in different interpreters.

If we used the {\tt array} method to create an array from a list, such as:

\begin{lstlisting}
a = array([[1,2],[3,4]])
\end{lstlisting}

we'd have the following bytecode program:

\begin{lstlisting}
  0  load array
  1  pushi 1
  2  pushi 2
  3  createList 2
  4  pushi 3
  5  pushi 4
  6  createList 2
  7  createList 2
  8  call
  9  store a
\end{lstlisting}

\subsubsection*{User Functions}

With the bytecodes so far, we can deal with loops, conditionals, indexing and local variables in user functions. User functions have very modest requires for bytecode support. In fact user functions only need two bytecodes:

\begin{lstlisting}
call
ret
\end{lstlisting}

User functions were discussed in detail in part II but its worth summarising some elements of their implementation. {\tt call} is used to call a user function and expects the user function object to be on the stack followed by any user function arguments it needs. {\tt call} also includes the number of expected arguments in the bytecode operand, so that something like {\tt call 2}, means that this call was called with 2 arguments. The function object itself stores the number of argument it actually expects. Since we have the actual and expected number of arguments we can check if there are enough arguments in the first place to satisfy the function object and secondly it allows us to implement a simple form of variable argument support.

\subsubsection*{Module Support}

The final topic to discuss is module support. Modules include user defined modules or the built-in modules. When the console starts up a special module is created, called {\tt _main_}. All interaction at the console is with this module. When a new module is loaded, its name is loaded into the {\tt _main_} symbolTable. If the new module itself loads in another module, the name of the second module will be added to the new module's symbol table. Any number of modules can be loaded this way, Figure~\ref{{fig:ImportingModule1}}. The virtual machine also has the notion of a current module. Whenever code is executed by the virtual machine it is run within the context of the module it exists in. This gives the code access to the symbol tables of the module it resides in.

\begin{figure}[htpb]
\centering
\includegraphics[scale=0.65]{importmodules.pdf}
\caption{Modules importing other modules. Module references go into the symbol table.}
\label{fig:ImportingModule1}
\end{figure}

User defined modules are in practice just another file containing Rhodus code. The key difference is the ability to import modules so that all the variables and user functions are put into their own user space determined by the name of the module file. Access at the Rhodus language level is achieved using the period (or full stop) notation. For example, if we create a file called, {\tt bankDetails.rh} that contains the variable, {\tt bankBalance}, we would import the file using import:

\begin{lstlisting}
import bankDetails
\end{lstlisting}

After that we can access {\tt bankBalance} using the period syntax as follows:

\begin{lstlisting}
bankDetails.bankBalance
\end{lstlisting}

This is a convenient way to package up user functions and other information into their own name space. For example, there may be another module file called {\tt myMoney} that also uses a variable called {\tt bankBalance}. We can safely use both variables {\tt bankBalance} because we can quality each one with the module its associated with.

Rhodus also has a set of built-in modules which behave in the same way as user modules. For example we can import the math module:

\begin{lstlisting}
import math
\end{lstlisting}

then access its information, for example, the value of $\pi$:

\begin{lstlisting}
a = math.pi
\end{lstlisting}

The question here is what extra bytecodes do we need to support this kind of feature? It turns out to be quite simple and in fact we only need three additional bytecodes:

\begin{lstlisting}
import
loadAttr
storeAttr
\end{lstlisting}

The {\tt import} bytecode is used, unsurprisingly, by the import statement. It's used to make sure that whatever is imported gets it own module and that any code in the module is compiled into bytecode. It also adds the name of the module to the currently accessible symbol table. Thus using import from the console will insert a references to the module into the {\tt _main_} symbolTable.

The two other instructions, {\tt loadAttr} and {\tt storeAttr}, are used to deal with the period syntax. You may be asking why not reuse {\tt store} and {\tt load}? The {\tt load} instruction takes a single operand, the name of the symbol and attempts to access that symbol in the symbolTable that belongs to current module. When using something like {\tt bankDetails.bankBalance}, we will have the module object, {\tt bankDetails} on the stack and {\tt loadAttr} is expected to pop that value off the stack and use that to reference the symbol {\tt bankBalance}. Let's look at some examples.

For the statement:

\begin{lstlisting}
a = math.pi
\end{lstlisting}

we would generate the following bytecode:

\begin{lstlisting}
  0  load math
  1  loadAttr pi
  2  store a
\end{lstlisting}

The actually access to {\tt pi} involves two instructions, push the math object onto the stack using {\tt load} and calling {\tt loadAttr}. {\tt loadAttr} expects the stack to hold the module object. It will pop off the module object and use that module to access the symbol {\tt pi}. Once it accesses the symbol {\tt pi} it pushes whatever value it find onto the stack. {\tt store a} just pops off the value on the stack and stores it to symbol {\tt a}, this time however in the current module.

For a user defined module, let's say called {\tt lib}, the code is exactly the same. For example, assume the module {\tt lib} contains a variable {\tt a}. If we were to access the variable using:

\begin{lstlisting}
x = lib.a
\end{lstlisting}

the bytecode would be:

\begin{lstlisting}
  0  load lib
  1  loadAttr lib
  2  store a
\end{lstlisting}

which is exactly the same the import math example.  If we wanted to store a new value into {\tt lib.a}, for example:

\begin{lstlisting}
lib.a = 4.5
\end{lstlisting}

we'd generate the following bytecode:

\begin{lstlisting}
  0  pushd 4.5
  1  load lib
  2  storeAttr a
\end{lstlisting}

We push 4.5 onto the stack, then push the module object for {\tt lib}. Finally we use {\tt storeAttr} which pops the module object, and stores whatever it find next in the stack (4.5), to the symbol {\tt a} in module {\tt lib}.

\subsubsection*{More Complex Expressions}

We now have everything we need to express any Rhodus program we might write. Let's looks at some more complex examples. The fist example shows us copying the {\tt math.sin} function into a list, then calling the first function via indexing and the function call syntax:

\begin{lstlisting}
a = [math.sin]
x = a[0](3.14)
\end{lstlisting}

This code gets turned into the following byte code. I've split the code in two, one for each line:

\begin{lstlisting}
  // a = [math.sin]
  0  load math
  1  loadAttr sin
  2  createList 1
  3  store a

  // x = a[0](3.14)
  0  load a
  1  pushi 0
  2  lvecIdx
  3  pushd 3.14
  4  call
  5  store x
\end{lstlisting}

Knowing what we already know, the code shouldn't be difficult to understand. The first part loads the math module and pushes the function object associated with {\tt sin} onto the stack. {\tt createList} then pops of one item from the stack (it doesn't care what kind of object it is), creates the list which is then stores to a variable {\tt a} via {\tt store}. The second half does the indexing and function calling. It first loads the list which is stored in {\tt a} onto the stack. It then pushes, what will be the index, {\tt 0} onto the stack. The first big event is the {\tt lvecIdx} instruction. This pops off the index and the list object, and pushes whatever it finds at {\tt [0]} onto the stack. Finally, {\tt call} is executed which takes the current value on the stack (which will be the function object) and executes it. Lastly we store whatever the {\tt call} left on the stack into symbol {\tt a}.

What's happening here is that whenever the compiler comes across an indexing operation is emits a {\tt lvecIdx}, and whenever it comes across a {\tt ()} it emits a {\tt call}. This means we can create some crazy expressions such as the following:

\begin{lstlisting}
x = modules[2].func(1,2)[2,2](True)
\end{lstlisting}

If the object stored at {\tt [2,2]} is not a function then the call {\tt (True)} will issue a runtime error. The same applies if we try to index a non-indexable object. For example, if we do the following:

\begin{lstlisting}
>> a = 5
>> a()
\end{lstlisting}

Clearly variable {\tt a} doesn't hold a function object we can call, the second line therefore results in:

{\tt\small ERROR: integer is not something that can be called as a function}

The same happens is we try this: {\tt a[0]} giving the message:

{\tt\small ERROR: integer variable is not indexable}

In conclusion there are not that many specialist opcodes. The majority are related to arithmetic or boolean operations. The full list of bytecodes is given in an appendix.


\section{Compiling Code}

Now that we've seen examples of bytecode we would generate for various Rhodus constructs, how do we generate such bytecode sequences? This topic returns us to the parsing stage. Here I want to say a little bit about the syntax analysis and generation of the abstract syntax tree, Figure~\ref{fig:Syntax_AST_Interaction}. The figure shows that syntax analysis and the abstract syntax tree phase have been separated into two blocks. It's possible to combine these into one block where syntax analysis and AST generation occur simultaneously and originally this is what I had. However I was confronted with a messy problem. An AST is a tree structure, with nodes and leaves and a root. Each node and leaf requires an allocation of memory to store the details pertinent to the node or leaf. ASTs vary in size depending on the size of the source code we parse, hence the AST has to be dynamically built on a need basis. Traditionally this is done by requesting memory from the heap. Once we've built the AST and eventually no long need it, the memory must be released. This is straightforward and involves traversing the tree and freeing the nodes and leaves one at a time.

\begin{figure}[htpb]
\centering
\includegraphics[scale=0.62]{flowOfInfo.pdf}
\caption{Stages in Version 3. The syntax analysis stage will cache the tokens it gets from the lexer which will be reused by the abstract syntax tree stage. }
\label{fig:Syntax_AST_Interaction}
\end{figure}

A problem arises however if we do syntax analysis and AST construction at the same time. That is, as we recognize syntactically correct pieces of source code we allocate memory to create nodes and leaves. What however, if there is a syntax error? We are now in a position where we have a partially constructed tree with most likely incomplete nodes. In this situation we have to report the error to the user and dismantle the current state of the AST. The solution I came up was  when are error occurred I would populate the AST at that point with error node. This meant I no longer had an incomplete tree and instead had error nodes which I could deal with later. However, this involved multiple checks at every stage and the code because unnecessarily messy and likely difficult to maintain. The solution to this was to separate syntax analysis from AST construction. In the syntax stage, the only thing that is done is to check syntax, no memory allocation is involved and if an error is detected we can get out without having to worry about unwinding complex data structures. If the syntax analysis passes without incident we then ask the AST phase to parse the source code again knowing that there will be no errors forthcoming. We don't have to worry about unwinding any data structures in the event of an error. Obviously there could be errors such as out of memory errors but if these happen, we're toast anyway, and at that point we should exit the interpreter completely.

Another advantage of separating syntax analysis from AST construction is that we can start to think about doing more sophisticated error handling at the syntax stage. At the moment, if we detect any error we just return immediately to the user. I'm not sure what this way of handling errors is called but we could call it the `I give up' approach. An alternative, which sounds equally bad is called `panic mode'. In panic model if the parser comes across something it doesn't understand it will try to lookahead to find something it does understand, and then continue from there. This allows to parser to report multiple errors if they exist in the source code, rather than a single error message as we do now. One disadvantage is that we run the risk of overwhelming the human operator with multiple, possibly conflicting, error messages all at once.

For now we'll use the `I give up' strategy but we'll revisit panic mode at a later time since its a common way to handle syntax errors.

I want to return now to the split between syntax and AST construction. You may have wondered that since we're reading the source code twice, once for syntax and another for the AST, isn't this inefficient? I suppose it is and it concerned me enough to at least reduce its impact. You'll notice in Figure~\ref{fig:Syntax_AST_Interaction} that the arrow between the syntax block and AST block has the words `Cached Tokens'. What this is meant to indicate is that when we do the syntax analysis we will cache the tokens we've read. To do this we don't have to change the lexical scanner it self, {\tt uScanner.pas}. Instead, in the syntax analysis phase all requests to the scanner method {\tt nextToken} are routed through a single {\tt nextToken} method in the syntax analysis code, {\tt uSyntaxAnalysis.pas}. This method looks like:

\begin{lstlisting}
procedure TSyntaxParser.nextToken;
begin
  sc.nextToken;
  tokenVector.append (sc.tokenElement);
end;
\end{lstlisting}

This method calls the scanner {\tt nextToken()} method but also appends the new token to a token cache held in a variable called {\tt tokenVector}. Other than this method, the syntax analysis has no other interaction with the lexical scanner. Instead it uses the {\tt tokenVector}. At the end of the syntax analysis, {\tt tokenVector} will contain a complete record of the token stream.

The AST construction code in kept in {\tt uConstruutAST.pas} and the constructor for the AST construction object takes as an argument the tokenVector. When parsing the source code to build the AST, it will use the cached tokens in tokenVector rather than parse the source code from scratch again. Although I've not compared timings, this should improve performance.

One issue that some of you might be wondering is how much space will the token cache take up? Each token requires at minimum 32 byte. Any extra space will be required to store literal strings. Let's assume for argument sake that on average each token will occupy 40 bytes. The biggest file in the test suite (arith1), can be translated into 1390 token, that requires 56K of storage. By today's standards that's a very modest file size. Even if we had a file that translated into 10,000 tokens that's 400K. Given that this memory requirement is temporary, the memory burden doesn't seem significant.

In the remaining chapters we will cover in more detail the various stages of the interpreter starting with syntax analysis.

%\section{Useful Reading}
%
%\subsection{Introductory Books}
%
%{\bf 1.} Ball, Thorsten. Writing A Compiler In Go. Thorsten Ball, 2018.
%
%{\bf 2.} Kernighan, Brian W.; Pike, Rob (1984). The Unix Programming Environment. Prentice-Hall. ISBN 0-13-937681-X.
%
%{\bf 3.} Nisan, Noam, and Shimon Schocken. The elements of computing systems: building a modern computer from first principles. MIT press, 2005.
%
%{\bf 4.} Parr, Terence. Language implementation patterns: create your own domain-specific and general programming languages. Pragmatic Bookshelf, 2009.
%
%\subsection{More Advanced Books}
%
%{\bf 1.} Jim Smith, Ravi Nair, Virtual Machines: Versatile Platforms for Systems and Processes, Morgan Kauffmann, June 2005
%
%{\bf 2.} Aho, Alfred V., Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques and Tools (also known as The Red Dragon Book), 1986.
%
%\subsection{Source Code}
%
%{\bf 1.} Mak, Ronald. Writing compilers and interpreters: an applied approach/by Ronald Mark. 1991
%
%Note, this is the first edition, 1991. The code is in C, which I found to be understandable. The later editions that use C++ are not as clear. The issue I found is that the object orientated approach that's used tends to obscure the design principles of the interpreter and requires much study to decipher, The C version is much more straightforward.
%
%{\bf 2.} Wren: \url{https://github.com/wren-lang/wren}.\index{wren}
%
%Of the open source interpreters on GitHub, I found this to be the easiest to read. It's written by Bob Nystrom in C, the same person who is writing the web book: Crafting Interpreters \url{https://craftinginterpreters.com/}.
%
%{\bf 3.} Gravity: \index{Gravity} Another open source interpreter worth looking at is Gravity (\url{https://github.com/marcobambini/gravity}). Gravity, like Wren, is also written in C.
%
%{\bf 4.}  If you prefer Go,\index{Go} then the source code to look at is the interpreter written by Thorsten Ball (see book reference above).
%
%There are umpteen BASIC interpreters\index{BASIC} and other languages that can be studied.
%
%\bigskip\medskip

\begin{center}
\pgfornament[width = 8cm, color = cardinal]{83}
\end{center} 